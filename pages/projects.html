<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Ekansh Sharma - Projects</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Arvo:ital,wght@0,400;0,700;1,400;1,700&display=swap"
        rel="stylesheet">
    <link rel="stylesheet" href="../css/styles.css">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>

<body>
    <!-- Navigation -->
    <nav class="navbar">
        <div class="nav-container">
            <div class="nav-logo">Ekansh Sharma</div>
            <ul class="nav-links">
                <li><a href="home.html">Home</a></li>
                <li><a href="education.html">Education</a></li>
                <li><a href="experience.html">Experience</a></li>
                <li><a href="projects.html">Projects</a></li>
                <!-- <li><a href="skills.html">Skills</a></li> -->
                <li><a href="publications.html">Publications</a></li>
                <li><a href="awards.html">Awards</a></li>
            </ul>
            <button class="hamburger" aria-label="Menu">
                <span></span>
                <span></span>
                <span></span>
            </button>
        </div>
    </nav>

    <!-- Mobile Sidebar -->
    <div class="sidebar-overlay"></div>
    <nav class="sidebar">
        <ul class="sidebar-links">
            <li><a href="home.html">Home</a></li>
            <li><a href="education.html">Education</a></li>
            <li><a href="experience.html">Experience</a></li>
            <li><a href="projects.html" class="active">Projects</a></li>
            <li><a href="publications.html">Publications</a></li>
            <li><a href="awards.html">Awards</a></li>
        </ul>
    </nav>

    <!-- Projects Section -->
    <section id="projects" class="projects-section">
        <h2 class="section-title">Projects & Open Source Contributions (OSC)</h2>

        <div class="projects-container">
            <!-- Master's Thesis -->
            <div class="project-item">
                <h3 class="project-title">Master's Thesis (Oct 2024 - Apr 2025)</h3>

                <div class="project-content">
                    <div class="project-info">
                        <div class="project-description">
                            <p> My master's thesis, <em>Model-based Optimal Estimation of Articulated Object
                                    Configuration Using Proprioception</em>, was conducted in joint cooperation with
                                <a href="https://www.h-brs.de/de/ia2s" target="_blank"
                                    rel="noopener noreferrer">Hochschule
                                    Bonn-Rhein-Sieg (H-BRS)<i class="fa fa-external-link"></i></a> and
                                <a href="https://www.idiap.ch/" target="_blank" rel="noopener noreferrer">the Idiap
                                    Research Institute (Idiap)<i class="fa fa-external-link"></i></a>, under the
                                supervision of <em>Prof. Dr. Sebastian
                                    Houben</em> (H-BRS), <em>Prof. Dr. Maren
                                    Bennewitz</em> (<a href="https://www.uni-bonn.de/" target="_blank"
                                    rel="noopener noreferrer">University of Bonn<i
                                        class="fa fa-external-link"></i></a>), <em>Dr. Sylvain Calinon</em> (Idiap), and
                                <em>Dr. Alex
                                    Mitrevski</em> (H-BRS).
                            </p>
                            <p>
                                In this work, I have developed a novel model-based method that enables a robot to
                                estimate
                                the optimal
                                kinematic configuration of articulated objects (such as doors, drawers, etc.) using
                                proprioceptive feedback during
                                physical
                                interaction, and subsequently use this information for informed motion planning and
                                compliant control,
                                resulting in efficient manipulation of these objects.
                            </p>
                            <p>
                                The approach resulted in up to 50-80% reduction in residual forces compared to a
                                baseline static estimation method and
                                is generalizable to other objects with similar kinematic structures.
                            </p>
                        </div>
                        <div class="project-links">
                            <span class="wip-button">WIP</span>
                        </div>
                    </div>
                    <div class="project-media">
                        <div class="media-item">
                            <div class="video-container">
                                <video controls preload="metadata" autoplay muted loop>
                                    <source src="../media/videos/project_masterthesis.webm" type="video/webm">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
            </div>

            <!-- OSC to Robotics Codes From Scratch -->
            <div class="project-item">
                <h3 class="project-title">OSC to Robotics Codes From Scratch (Sept 2024)</h3>

                <div class="project-content">
                    <div class="project-info">
                        <div class="project-description">
                            <p>
                                Contributed an example implementation of trajectory optimization with boundary
                                constraints using iLQR for manipulator
                                control to the <em>Robotics Codes From Scratch</em> (RCFS) repository, maintained by my
                                thesis supervisor, <em>Dr. Sylvain Calinon.
                                </em>
                            </p>

                            <p>
                                This example was originally developed to investigate two key research questions during
                                my preliminary thesis work: (i)
                                how boundary constraints can be applied in iLQR-based optimization, and (ii) whether
                                they improve convergence
                                performance in subsequent thesis experiments.
                            </p>

                            <p>
                                The implementation was later integrated into RCFS, marking my first contribution to the
                                repository and serving as an
                                educational resource for the broader robotics community.
                            </p>
                        </div>
                        <div class="project-links">
                            <a href="https://gitlab.idiap.ch/rli/robotics-codes-from-scratch/-/blob/master/python/iLQR_manipulator_boundary.py?ref_type=heads"
                                target="_blank">GitLab</a>
                        </div>
                    </div>
                    <div class="project-media">
                        <div class="media-item">
                            <div class="video-container">
                                <video controls preload="metadata" autoplay muted loop>
                                    <source src="../media/videos/project_rcfs.mp4" type="video/mp4">
                                    Your browser does not support the video tag.
                                </video>
                            </div>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Bag of Visual Words Implementation -->
        <div class="project-item">
            <h3 class="project-title">Bag of Visual Words in C++ (Feb 2024 - Mar 2024)</h3>

            <div class="project-content">
                <div class="project-info">
                    <div class="project-description">
                        <div class="project-description">
                            <p>
                                Implemented the Bag of Visual Words algorithm from scratch using modern C++ for a
                                content-based image retrieval system,
                                as the final project for the <em>Modern C++ for Computer Vision</em> course at the
                                University of Bonn.
                            </p>
                            <p>
                                The project featured a modular architecture with components for feature extraction,
                                k-means dictionary construction,
                                histogram computation, similarity matching using cosine distance and TF-IDF weighting,
                                and HTML-based result
                                visualization—all organized within a comprehensive CMake build system.
                            </p>
                            <p>
                                According to the course instructor, I was the first student to successfully complete
                                this project, demonstrating strong
                                proficiency in both modern C++ practices and core computer vision algorithms.
                            </p>
                        </div>
                    </div>
                    <div class="project-links">
                        <a href="https://github.com/Ekanshh/bag-of-visual-words" target="_blank">GitHub</a>
                    </div>
                </div>
                <div class="project-media">
                    <div class="media-item">
                        <div class="video-container">
                            <img src="../media/images/project_bovw.png" alt="Bag of Visual Words Demo"
                                class="clickable-image" onclick="openImageModal(this.src)">
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Learning trajectory and task constraints from demonstrations -->
        <div class="project-item">
            <h3 class="project-title">Learning Trajectory and Task Constraints from Demonstrations (May 2023 - Jun 2023)
            </h3>

            <div class="project-content">
                <div class="project-info">
                    <div class="project-description">
                        <p>
                            Three-person group project for the <em>Cognitive Robotics</em> course at H-BRS, where we
                            developed an imitation learning system
                            for a cube stacking task, based on the Learning from Demonstration (LfD) paradigm.
                        </p>
                        <p>
                            Our approach combined trajectory learning via Gaussian Mixture Models and Gaussian Mixture
                            Regression (GMM-GMR) with
                            task constraint modeling using directed graphs. The system effectively learns the trajectory
                            execution skill and adapts
                            it to new start and goal positions through geometric transformations, while capturing
                            task-specific constraints, such as
                            stacking order, within a graph structure.
                        </p>
                        <p>
                            We demonstrated the method using a 3D visualization, showing successful
                            generalization to different
                            cube configurations while preserving the learned task constraints. The approach serves as a
                            foundation for extension to
                            real robotic systems.
                        </p>
                    </div>
                    <div class="project-links">
                        <a href="https://github.com/Ekanshh/learning-trajectory-and-task-constraints-from-demonstrations"
                            target="_blank">GitHub</a>
                    </div>
                </div>
                <div class="project-media">
                    <div class="media-item">
                        <div class="video-container">
                            <video controls preload="metadata" autoplay muted loop>
                                <source src="../media/videos/project_gmmgmr.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Robot Learning and Control -->
        <div class="project-item">
            <h3 class="project-title">R&D Project (May 2022 - Jan 2023)</h3>

            <div class="project-content">
                <div class="project-info">
                    <div class="project-description">
                        <p>
                            This R&D project, <em>Development and implementation of a self-learning control
                                approach
                                for contact-rich object manipulation</em>, was conducted in joint cooperation with H-BRS
                            and
                            <a href="https://ifu.rwth-aachen.de/" target="_blank" rel="noopener noreferrer">Institut für
                                Unternehmenskybernetik (IfU) e.V.<i class="fa fa-external-link"></i></a>, under the
                            supervision of <em>Prof. Dr. Paul G. Plöger</em> (H-BRS), <em>Dr. Alex Mitrevski</em>
                            (H-BRS),
                            and <em>Dr. Christoph Henke</em> (IfU).
                        </p>

                        <p>
                            In this work, I developed an adaptive compliant control approach that enables a
                            manipulator to learn and reliably
                            execute object press-fitting tasks in shipping container loading, using a single human
                            demonstration augmented with
                            interactive user corrections. The system incorporates proprioceptive feedback-based failure
                            recovery mechanism to
                            autonomously monitor execution, detect collisions, and initiate corrective behaviors.
                        </p>

                        <p>
                            The approach was evaluated on a miniature container loading setup under systematic
                            variations in start positions, goal
                            configurations, and object grasping poses, demonstrating improved adaptability and collision
                            recovery over a baseline
                            framework. This work was subsequently published at the European Conference on Mobile Robots
                            (ECMR) 2023.

                        </p>
                        See the <button class="btn-nav-style" onclick="window.location.href='publications.html'">
                            Publications</button> section for more details.
                    </div>
                </div>


                <div class="project-media">
                    <div class="media-item">
                        <div class="video-container">
                            <iframe
                                src="https://www.youtube.com/embed/cFChda1Pccc?autoplay=1&mute=1&loop=1&playlist=cFChda1Pccc"
                                frameborder="0"
                                allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
                                allowfullscreen>
                            </iframe>
                        </div>
                    </div>
                </div>
            </div>
        </div>

        <!-- Computer Vision Projects -->
        <div class="project-item">
            <h3 class="project-title">OSC to ROS 2 Nav2 (DEC 2021 — FEB 2022)</h3>

            <div class="project-content">
                <div class="project-info">
                    <p class="project-description">
                        I contributed to the official ROS 2 Nav2 repository by extending
                        the Python API with a new method that enables direct controller
                        action server calls with custom paths, along with a supporting demo. This enhancement allows
                        Python developers to bypass the standard planning pipeline and execute pre-computed or
                        custom-generated paths directly, providing greater programmatic control.
                    </p>
                    <div class="project-links">
                        <a href="https://github.com/ros-navigation/navigation2/pull/2789" target="_blank">GitHub</a>
                    </div>
                </div>
                <div class="project-media">
                    <div class="media-item">
                        <div class="video-container">
                            <img src="https://raw.githubusercontent.com/ros-navigation/navigation2/main/doc/nav2_logo.png"
                                alt="Nav2 Logo" class="clickable-image nav2-logo" onclick="openImageModal(this.src)">
                        </div>
                        <figcaption>
                            Logo courtesy of the ROS 2 Nav2 project. Source: <a
                                href="https://github.com/ros-navigation/navigation2" target="_blank"
                                rel="noopener noreferrer"> Nav2 official GitHub repository <i
                                    class="fa fa-external-link"></i></a>
                        </figcaption>
                    </div>
                </div>
            </div>
        </div>

        <div class="project-item">
            <h3 class="project-title"> Configuring KUKA youBot with ROS Navigation Stack
                from Scratch (Aug 2021)</h3>

            <div class="project-content">
                <div class="project-info">
                    <div class="project-description">
                        <p>
                            The objective of this project was to gain a deep
                            understanding of how to configure a mobile robot
                            with ROS navigation stack from scratch, and to provide a detailed, step-by-step
                            tutorial to assist newcomers at b-it-bots@Work Lab at H-BRS.
                        </p>
                        <p>
                            Co-authored a comprehensive tutorial that includes setting up the KUKA
                            youBot platform with an external laptop, covering hardware configuration, driver
                            installation, integration of Hokuyo Lidar sensors, and implementation of SLAM-based mapping
                            and AMCL localization.
                        </p>
                    </div>
                    <div class="project-links">
                        <a href="https://github.com/Ekanshh/youBot_from_scratch" target="_blank">GitHub</a>
                    </div>
                </div>

                <div class="project-media">
                    <div class="media-item">
                        <figure>
                            <div class="video-container">
                                <img src="https://www.h-brs.de/sites/default/files/styles/hresize922x520_57/public/2023-01/youbot.png.webp?itok=RDRzIX2p&t=9ea0"
                                    alt="KUKA youBot robot platform at Hochschule Bonn-Rhein-Sieg"
                                    class="clickable-image" onclick="openImageModal(this.src)">
                            </div>
                            <figcaption>
                                KUKA youBot as used for research at Hochschule Bonn-Rhein-Sieg. Source: <a
                                    href="https://www.h-brs.de/de/a2s/roboter" target="_blank"
                                    rel="noopener noreferrer">Hochschule
                                    Bonn-Rhein-Sieg official website <i class="fa fa-external-link"></i></a>
                            </figcaption>
                        </figure>
                    </div>
                </div>
            </div>
        </div>

        <div class="project-item">
            <h3 class="project-title">Home Service Robot (Apr 2020)</h3>

            <div class="project-content">
                <div class="project-info">
                    <div class="project-description">
                        <p>I developed a home service robot simulation as the capstone project for the Udacity Robotics
                            Software Nanodegree
                            Program, where a TurtleBot performs autonomous item delivery operations in a Gazebo
                            environment.</p>

                        <p>The implementation involved deploying a TurtleBot in a custom Gazebo world where the robot
                            first maps the environment
                            through teleoperation using GMapping's laser-based SLAM to generate 2D occupancy grid maps.
                            The system then
                            integrates AMCL for localization and Dijkstra's algorithm for path planning with dynamic
                            obstacle avoidance,
                            enabling autonomous navigation to pickup and delivery locations.</p>
                    </div>
                    <div class="project-links">
                        <a href="https://github.com/Ekanshh/rse_p7_udacity" target="_blank">GitHub</a>
                    </div>
                </div>

                <div class="project-media">
                    <div class="media-item">
                        <div class="video-container">
                            <video controls preload="metadata" autoplay muted loop>
                                <source src="../media/videos/project_homeservicerobot.mp4" type="video/mp4">
                                Your browser does not support the video tag.
                            </video>
                        </div>
                    </div>
                </div>
            </div>
        </div>
        </div>
    </section>

    <!-- Image Modal -->
    <div id="imageModal" class="image-modal">
        <span class="close-modal">&times;</span>
        <img id="modalImage" class="modal-content">
    </div>

    <!-- Footer -->
    <footer class="footer">
        <div class="footer-content">
            <p>Made with
                <a href="https://www.cursor.sh" target="_blank" rel="noopener noreferrer">
                    Cursor
                </a>
                ❤️
            </p>
            <p>&copy; 2025 Ekansh Sharma. All rights reserved.</p>
        </div>
    </footer>

    <script src="../js/main.js"></script>

    <script>
        function openImageModal(imageSrc) {
            const modal = document.getElementById('imageModal');
            const modalImg = document.getElementById('modalImage');
            modal.style.display = 'block';
            modalImg.src = imageSrc;
        }

        // Close modal when clicking the X
        document.querySelector('.close-modal').onclick = function () {
            document.getElementById('imageModal').style.display = 'none';
        }

        // Close modal when clicking outside the image
        document.getElementById('imageModal').onclick = function (e) {
            if (e.target === this) {
                this.style.display = 'none';
            }
        }

        // Close modal with Escape key
        document.addEventListener('keydown', function (e) {
            if (e.key === 'Escape') {
                document.getElementById('imageModal').style.display = 'none';
            }
        });
    </script>
</body>

</html>